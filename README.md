# Contextual
A CLI for running local LLMs with Ollama
